"""
æ™ºèƒ½è§†é¢‘åˆæˆå™¨ï¼ˆAIé©±åŠ¨ï¼‰
æ•´åˆè¯­ä¹‰åˆ†æã€è½¬åœºå†³ç­–å’ŒKen Burnsæ•ˆæœçš„æ™ºèƒ½åˆæˆç³»ç»Ÿ
"""

import json
import os
import sys
from typing import Dict, Any, List, Optional
from datetime import datetime

# å¯¼å…¥åŸºç¡€åˆæˆå™¨
sys.path.insert(0, os.path.dirname(__file__))
from composer import VideoComposer

# å¯¼å…¥æ™ºèƒ½ç»„ä»¶
from semantic_analyzer import SemanticAnalyzer
from transition_engine import TransitionDecisionEngine
from ken_burns import KenBurnsGenerator
from transitions import TransitionLibrary

# å¯¼å…¥AIå®¢æˆ·ç«¯
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '1_script_generator'))
from ai_client import AIClient


class SmartVideoComposer(VideoComposer):
    """æ™ºèƒ½è§†é¢‘åˆæˆå™¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""

    def __init__(self, config_path: str = 'config/settings.json'):
        """
        åˆå§‹åŒ–æ™ºèƒ½åˆæˆå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        super().__init__(config_path)

        # åŠ è½½æ™ºèƒ½åŠ¨æ•ˆé…ç½®
        self.smart_config = self.config.get('smart_effects', {})
        self.smart_enabled = self.smart_config.get('enable', True)
        self.use_ai_analysis = self.smart_config.get('use_ai_analysis', False)
        self.ken_burns_enabled = self.smart_config.get('ken_burns_enabled', True)

        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        ai_client = None
        if self.use_ai_analysis:
            try:
                ai_client = AIClient(self.config['ai'])
            except Exception as e:
                print(f"   âš ï¸  AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                print(f"   â†’ é™çº§åˆ°è§„åˆ™å¼•æ“æ¨¡å¼")
                self.use_ai_analysis = False

        # åˆå§‹åŒ–æ™ºèƒ½ç»„ä»¶
        self.semantic_analyzer = SemanticAnalyzer(
            ai_client=ai_client,
            use_ai=self.use_ai_analysis
        )
        self.transition_engine = TransitionDecisionEngine()
        self.ken_burns = KenBurnsGenerator()

        print(f"\nğŸ§  æ™ºèƒ½åŠ¨æ•ˆç³»ç»Ÿ: "
              f"{'å·²å¯ç”¨' if self.smart_enabled else 'å·²ç¦ç”¨'}")
        if self.smart_enabled:
            print(f"   AIåˆ†æ: {'å·²å¯ç”¨' if self.use_ai_analysis else 'è§„åˆ™å¼•æ“'}")
            print(f"   Ken Burns: {'å·²å¯ç”¨' if self.ken_burns_enabled else 'å·²ç¦ç”¨'}")

    def compose_from_script(
        self,
        script: Dict[str, Any],
        auto_select_materials: bool = True,
        output_filename: Optional[str] = None,
        tts_metadata_path: Optional[str] = None,
        subtitle_file: Optional[str] = None,
        use_tts_audio: bool = True
    ) -> str:
        """
        æ™ºèƒ½åˆæˆè§†é¢‘ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼‰

        æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ï¼š
        - smart_enabled=True: ä½¿ç”¨æ™ºèƒ½ç³»ç»Ÿ
        - smart_enabled=False: ä½¿ç”¨åŸå§‹æ–¹æ³•

        Args:
            script: è„šæœ¬å­—å…¸
            auto_select_materials: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©ç´ æ
            output_filename: è¾“å‡ºæ–‡ä»¶å
            tts_metadata_path: TTSå…ƒæ•°æ®è·¯å¾„
            subtitle_file: å­—å¹•æ–‡ä»¶è·¯å¾„
            use_tts_audio: æ˜¯å¦ä½¿ç”¨TTSéŸ³é¢‘

        Returns:
            è§†é¢‘æ–‡ä»¶è·¯å¾„
        """
        if not self.smart_enabled:
            # é™çº§åˆ°åŸå§‹æ–¹æ³•
            return super().compose_from_script(
                script,
                auto_select_materials,
                output_filename,
                tts_metadata_path,
                subtitle_file,
                use_tts_audio
            )

        # ä½¿ç”¨æ™ºèƒ½åˆæˆ
        return self._smart_compose(
            script,
            auto_select_materials,
            output_filename,
            tts_metadata_path,
            subtitle_file,
            use_tts_audio
        )

    def _smart_compose(
        self,
        script: Dict[str, Any],
        auto_select_materials: bool,
        output_filename: Optional[str],
        tts_metadata_path: Optional[str],
        subtitle_file: Optional[str],
        use_tts_audio: bool
    ) -> str:
        """æ™ºèƒ½åˆæˆæ ¸å¿ƒé€»è¾‘"""

        if not self.editor.moviepy_available:
            raise ImportError("moviepyæœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install moviepy")

        from moviepy.editor import (
            ImageClip, VideoFileClip, TextClip, CompositeVideoClip,
            concatenate_videoclips, AudioFileClip
        )

        print(f"\nğŸ¬ æ™ºèƒ½è§†é¢‘åˆæˆ: {script.get('title', 'æœªå‘½å')}")
        print("=" * 70)

        sections = script.get('sections', [])
        if not sections:
            raise ValueError("è„šæœ¬æ²¡æœ‰ç« èŠ‚å†…å®¹")

        # ========== ç¬¬1æ­¥: AIè¯­ä¹‰åˆ†æ ==========
        print(f"\nğŸ§  AIè¯­ä¹‰åˆ†æä¸­...")
        analyses = self.semantic_analyzer.analyze_all_sections(sections)

        # æ‰“å°åˆ†æç»“æœæ‘˜è¦
        print(f"\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
        for i, (section, analysis) in enumerate(zip(sections, analyses), 1):
            print(f"   {i}. {section.get('section_name', 'N/A')}: "
                  f"èƒ½é‡{analysis['energy_level']:.1f}, "
                  f"æƒ…ç»ª{analysis['emotion']}")

        # ========== ç¬¬2æ­¥: ç”Ÿæˆå¸¦åŠ¨æ•ˆçš„clips ==========
        all_clips = []
        temp_clips = []
        audio_clips = []

        print(f"\nğŸ¨ ç”Ÿæˆè§†é¢‘ç‰‡æ®µ...")

        for i, (section, analysis) in enumerate(zip(sections, analyses), 1):
            section_name = section.get('section_name', f'ç« èŠ‚{i}')
            print(f"\nğŸ“ å¤„ç†ç« èŠ‚ {i}/{len(sections)}: {section_name}")

            # 2.1 è·å–ç´ æ
            material_path = self._get_material_for_section(
                section,
                auto_select_materials
            )

            duration = section.get('duration', self.default_image_duration)

            # 2.2 åˆ›å»ºåŸºç¡€clip
            clip = self._create_base_clip(material_path, duration)

            # 2.3 åº”ç”¨Ken Burnsæ•ˆæœï¼ˆå¦‚æœæ˜¯é™æ€å›¾ç‰‡ï¼‰
            if self.ken_burns_enabled and material_path:
                ext = os.path.splitext(material_path)[1].lower()
                if ext in ['.jpg', '.png', '.jpeg', '.gif']:
                    movement_type = self.ken_burns._decide_movement_type(
                        analysis['energy_level'],
                        analysis['emotion']
                    )
                    clip = self.ken_burns.apply_ken_burns(clip, analysis, duration)
                    print(f"   âœ¨ Ken Burns: {movement_type}")

            # 2.4 å†³å®šè½¬åœºï¼ˆé™¤ç¬¬ä¸€ä¸ªclipï¼‰
            if i > 0:
                transition = self.transition_engine.decide_transition(
                    analyses[i-1],
                    analysis,
                    analyses[i+1] if i < len(analyses)-1 else None
                )

                print(f"   ğŸ¬ è½¬åœº: {transition['type']} "
                      f"({transition['duration']}s)")
                print(f"      ç†ç”±: {transition['reason']}")

                # åº”ç”¨è½¬åœº
                clip = self._apply_transition(clip, transition)

            # 2.5 æ·»åŠ æ–‡å­—ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
            narration = section.get('narration', '')
            if self.video_config.get('show_narration_text', False) and narration:
                text_clip = self._create_text_clip(
                    narration,
                    duration=duration,
                    position=('center', 'bottom'),
                    fontsize=self.video_config.get('text_size', 40)
                )
                clip = CompositeVideoClip([clip, text_clip])

            all_clips.append(clip)

        if not all_clips:
            raise ValueError("æ²¡æœ‰ç”Ÿæˆä»»ä½•è§†é¢‘ç‰‡æ®µ")

        # ========== ç¬¬3æ­¥: åˆå¹¶clips ==========
        print(f"\nğŸï¸  åˆå¹¶ {len(all_clips)} ä¸ªç‰‡æ®µ...")
        final_video = concatenate_videoclips(all_clips, method="compose")

        # ========== ç¬¬4æ­¥: æ·»åŠ éŸ³é¢‘ ==========
        final_video = self._add_audio(
            final_video,
            use_tts_audio,
            tts_metadata_path,
            audio_clips
        )

        # ========== ç¬¬5æ­¥: æ·»åŠ å­—å¹• ==========
        if subtitle_file and os.path.exists(subtitle_file):
            final_video = self._add_subtitles(final_video, subtitle_file)

        # ========== ç¬¬6æ­¥: å¯¼å‡ºè§†é¢‘ ==========
        output_path = self._export_video(
            final_video,
            script,
            output_filename,
            len(all_clips)
        )

        # ========== ç¬¬7æ­¥: æ¸…ç†èµ„æº ==========
        self._cleanup_resources(all_clips, audio_clips, final_video)

        return output_path

    def _get_material_for_section(
        self,
        section: Dict[str, Any],
        auto_select: bool
    ) -> Optional[str]:
        """è·å–ç« èŠ‚ç´ æ"""
        if auto_select:
            print("   ğŸ” æ™ºèƒ½æ¨èç´ æ...")
            recommendations = self.recommender.recommend_for_script_section(
                section,
                limit=3
            )

            if recommendations:
                best_material = recommendations[0]
                material_path = best_material['file_path']
                print(f"   âœ… é€‰æ‹©: {best_material['name']} "
                      f"(åŒ¹é…åº¦: {best_material.get('match_score', 0):.0f}%)")
                return material_path
            else:
                print("   âš ï¸  æœªæ‰¾åˆ°åˆé€‚ç´ æï¼Œä½¿ç”¨é»˜è®¤é»‘å±")
                return None
        else:
            return None

    def _create_base_clip(self, material_path: Optional[str], duration: float):
        """åˆ›å»ºåŸºç¡€clip"""
        from moviepy.editor import ImageClip, VideoFileClip

        if material_path and os.path.exists(material_path):
            ext = os.path.splitext(material_path)[1].lower()
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:
                return ImageClip(material_path).set_duration(duration)
            elif ext in ['.mp4', '.avi', '.mov', '.mkv']:
                video_clip = VideoFileClip(material_path)
                if video_clip.duration < duration:
                    return video_clip.loop(duration=duration)
                else:
                    return video_clip.subclip(0, duration)
            else:
                print(f"   âš ï¸  ä¸æ”¯æŒçš„æ ¼å¼: {ext}")
                return self._create_color_clip((0, 0, 0), duration)
        else:
            return self._create_color_clip((0, 0, 0), duration)

    def _apply_transition(self, clip, transition: Dict[str, Any]):
        """åº”ç”¨è½¬åœºæ•ˆæœåˆ°clip"""
        trans_type = transition['type']
        duration = transition['duration']
        params = transition.get('params', {})

        if trans_type == 'fade':
            return clip.fadein(duration)
        elif trans_type == 'zoom_in':
            zoom_ratio = params.get('zoom_ratio', 1.3)
            return TransitionLibrary.zoom_in(clip, duration, zoom_ratio)
        elif trans_type == 'zoom_out':
            zoom_ratio = params.get('zoom_ratio', 1.3)
            return TransitionLibrary.zoom_out(clip, duration, zoom_ratio)
        elif trans_type == 'slide_left':
            return TransitionLibrary.slide_in(clip, 'left', duration)
        elif trans_type == 'slide_right':
            return TransitionLibrary.slide_in(clip, 'right', duration)
        elif trans_type == 'hard_cut':
            return clip  # æ— è½¬åœº
        elif trans_type == 'crossfade':
            return clip.crossfadein(duration)
        else:
            return clip

    def _add_audio(
        self,
        video_clip,
        use_tts: bool,
        tts_metadata_path: Optional[str],
        audio_clips: list
    ):
        """æ·»åŠ éŸ³é¢‘ï¼ˆTTSæˆ–BGMï¼‰"""
        from moviepy.editor import AudioFileClip, concatenate_audioclips
        from moviepy.audio.AudioClip import CompositeAudioClip

        if use_tts and tts_metadata_path and os.path.exists(tts_metadata_path):
            print("\nğŸ™ï¸  æ·»åŠ TTSè¯­éŸ³...")
            try:
                with open(tts_metadata_path, 'r', encoding='utf-8') as f:
                    tts_metadata = json.load(f)

                audio_files = [item['file_path']
                              for item in tts_metadata.get('audio_files', [])]

                if audio_files:
                    tts_audio_clips = [AudioFileClip(f)
                                      for f in audio_files if os.path.exists(f)]
                    audio_clips.extend(tts_audio_clips)

                    if tts_audio_clips:
                        tts_audio = concatenate_audioclips(tts_audio_clips)

                        # æ·»åŠ BGMèƒŒæ™¯
                        bgm_path = self.video_config.get('default_bgm')
                        if bgm_path and os.path.exists(bgm_path):
                            bgm = AudioFileClip(bgm_path)
                            if bgm.duration < tts_audio.duration:
                                bgm = bgm.loop(duration=tts_audio.duration)
                            else:
                                bgm = bgm.subclip(0, tts_audio.duration)
                            bgm = bgm.volumex(0.2)
                            final_audio = CompositeAudioClip([tts_audio, bgm])
                        else:
                            final_audio = tts_audio

                        video_clip = video_clip.set_audio(final_audio)
                        print(f"   âœ… TTSéŸ³é¢‘å·²æ·»åŠ  (æ—¶é•¿: {tts_audio.duration:.1f}ç§’)")

                        if video_clip.duration != tts_audio.duration:
                            video_clip = video_clip.set_duration(tts_audio.duration)
            except Exception as e:
                print(f"   âš ï¸  æ·»åŠ TTSå¤±è´¥: {str(e)}")
        else:
            # æ·»åŠ BGM
            bgm_path = self.video_config.get('default_bgm')
            if bgm_path and os.path.exists(bgm_path):
                print("\nğŸµ æ·»åŠ èƒŒæ™¯éŸ³ä¹...")
                try:
                    audio = AudioFileClip(bgm_path)
                    if audio.duration < video_clip.duration:
                        audio = audio.loop(duration=video_clip.duration)
                    else:
                        audio = audio.subclip(0, video_clip.duration)
                    video_clip = video_clip.set_audio(audio)
                except Exception as e:
                    print(f"   âš ï¸  æ·»åŠ BGMå¤±è´¥: {str(e)}")

        return video_clip

    def _add_subtitles(self, video_clip, subtitle_file: str):
        """æ·»åŠ å­—å¹•"""
        print(f"\nğŸ“ æ·»åŠ å­—å¹•: {subtitle_file}")
        try:
            from moviepy.video.tools.subtitles import SubtitlesClip
            from moviepy.editor import TextClip, CompositeVideoClip

            def generator(txt):
                return TextClip(
                    txt,
                    fontsize=self.video_config.get('text_size', 48),
                    color='white',
                    bg_color='black',
                    method='caption',
                    size=(video_clip.w - 200, None)
                )

            subtitles = SubtitlesClip(subtitle_file, generator)
            video_clip = CompositeVideoClip([
                video_clip,
                subtitles.set_position(('center', 'bottom'))
            ])

            print("   âœ… å­—å¹•å·²æ·»åŠ ")
        except Exception as e:
            print(f"   âš ï¸  æ·»åŠ å­—å¹•å¤±è´¥: {str(e)}")

        return video_clip

    def _export_video(
        self,
        video_clip,
        script: Dict[str, Any],
        output_filename: Optional[str],
        clip_count: int
    ) -> str:
        """å¯¼å‡ºè§†é¢‘"""
        if output_filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            script_title = script.get('title', 'video')
            safe_title = "".join(c for c in script_title
                                if c.isalnum() or c in (' ', '-', '_')).strip()
            output_filename = f"{safe_title}_{timestamp}.mp4"

        output_path = os.path.join(self.editor.output_dir, output_filename)

        print(f"\nğŸ’¾ å¯¼å‡ºè§†é¢‘...")
        video_clip.write_videofile(
            output_path,
            fps=self.video_config.get('fps', 24),
            codec=self.video_config.get('codec', 'libx264'),
            audio_codec=self.video_config.get('audio_codec', 'aac')
        )

        print(f"\nâœ… æ™ºèƒ½è§†é¢‘åˆæˆå®Œæˆ!")
        print(f"   è¾“å‡º: {output_path}")
        print(f"   æ—¶é•¿: {video_clip.duration:.1f}ç§’")
        print(f"   ç‰‡æ®µæ•°: {clip_count}")

        return output_path

    def _cleanup_resources(self, all_clips: list, audio_clips: list, final_video):
        """æ¸…ç†èµ„æºé˜²æ­¢å†…å­˜æ³„æ¼"""
        print("\nğŸ§¹ æ¸…ç†ä¸´æ—¶èµ„æº...")
        try:
            for clip in all_clips:
                if clip and hasattr(clip, 'close'):
                    try:
                        clip.close()
                    except:
                        pass

            for clip in audio_clips:
                if clip and hasattr(clip, 'close'):
                    try:
                        clip.close()
                    except:
                        pass

            if hasattr(final_video, 'close'):
                try:
                    final_video.close()
                except:
                    pass
        except Exception as e:
            print(f"   âš ï¸  æ¸…ç†æ—¶å‡ºé”™: {str(e)}")
