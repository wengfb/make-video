"""
æ™ºèƒ½è§†é¢‘åˆæˆå™¨ï¼ˆAIé©±åŠ¨ï¼‰
æ•´åˆè¯­ä¹‰åˆ†æã€è½¬åœºå†³ç­–å’ŒKen Burnsæ•ˆæœçš„æ™ºèƒ½åˆæˆç³»ç»Ÿ
"""

import json
import os
import sys
from typing import Dict, Any, List, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# å¯¼å…¥åŸºç¡€åˆæˆå™¨
sys.path.insert(0, os.path.dirname(__file__))
from composer import VideoComposer

# å¯¼å…¥æ™ºèƒ½ç»„ä»¶
from semantic_analyzer import SemanticAnalyzer
from transition_engine import TransitionDecisionEngine
from ken_burns import KenBurnsGenerator
from transitions import TransitionLibrary

# å¯¼å…¥AIå®¢æˆ·ç«¯
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '1_script_generator'))
from ai_client import AIClient


class SmartVideoComposer(VideoComposer):
    """æ™ºèƒ½è§†é¢‘åˆæˆå™¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""

    def __init__(self, config_path: str = 'config/settings.json'):
        """
        åˆå§‹åŒ–æ™ºèƒ½åˆæˆå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        super().__init__(config_path)

        # åŠ è½½æ™ºèƒ½åŠ¨æ•ˆé…ç½®
        self.smart_config = self.config.get('smart_effects', {})
        self.smart_enabled = self.smart_config.get('enable', True)
        self.use_ai_analysis = self.smart_config.get('use_ai_analysis', False)
        self.ken_burns_enabled = self.smart_config.get('ken_burns_enabled', True)

        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        ai_client = None
        if self.use_ai_analysis:
            try:
                ai_client = AIClient(self.config['ai'])
            except Exception as e:
                print(f"   âš ï¸  AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                print(f"   â†’ é™çº§åˆ°è§„åˆ™å¼•æ“æ¨¡å¼")
                self.use_ai_analysis = False

        # åˆå§‹åŒ–æ™ºèƒ½ç»„ä»¶
        self.semantic_analyzer = SemanticAnalyzer(
            ai_client=ai_client,
            use_ai=self.use_ai_analysis
        )
        self.transition_engine = TransitionDecisionEngine()
        self.ken_burns = KenBurnsGenerator()

        print(f"\nğŸ§  æ™ºèƒ½åŠ¨æ•ˆç³»ç»Ÿ: "
              f"{'å·²å¯ç”¨' if self.smart_enabled else 'å·²ç¦ç”¨'}")
        if self.smart_enabled:
            print(f"   AIåˆ†æ: {'å·²å¯ç”¨' if self.use_ai_analysis else 'è§„åˆ™å¼•æ“'}")
            print(f"   Ken Burns: {'å·²å¯ç”¨' if self.ken_burns_enabled else 'å·²ç¦ç”¨'}")

    def compose_from_script(
        self,
        script: Dict[str, Any],
        auto_select_materials: bool = True,
        output_filename: Optional[str] = None,
        tts_metadata_path: Optional[str] = None,
        subtitle_file: Optional[str] = None,
        use_tts_audio: bool = True
    ) -> str:
        """
        æ™ºèƒ½åˆæˆè§†é¢‘ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼‰

        æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ï¼š
        - smart_enabled=True: ä½¿ç”¨æ™ºèƒ½ç³»ç»Ÿ
        - smart_enabled=False: ä½¿ç”¨åŸå§‹æ–¹æ³•

        Args:
            script: è„šæœ¬å­—å…¸
            auto_select_materials: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©ç´ æ
            output_filename: è¾“å‡ºæ–‡ä»¶å
            tts_metadata_path: TTSå…ƒæ•°æ®è·¯å¾„
            subtitle_file: å­—å¹•æ–‡ä»¶è·¯å¾„
            use_tts_audio: æ˜¯å¦ä½¿ç”¨TTSéŸ³é¢‘

        Returns:
            è§†é¢‘æ–‡ä»¶è·¯å¾„
        """
        if not self.smart_enabled:
            # é™çº§åˆ°åŸå§‹æ–¹æ³•
            return super().compose_from_script(
                script,
                auto_select_materials,
                output_filename,
                tts_metadata_path,
                subtitle_file,
                use_tts_audio
            )

        # ä½¿ç”¨æ™ºèƒ½åˆæˆ
        return self._smart_compose(
            script,
            auto_select_materials,
            output_filename,
            tts_metadata_path,
            subtitle_file,
            use_tts_audio
        )

    def _smart_compose(
        self,
        script: Dict[str, Any],
        auto_select_materials: bool,
        output_filename: Optional[str],
        tts_metadata_path: Optional[str],
        subtitle_file: Optional[str],
        use_tts_audio: bool
    ) -> str:
        """æ™ºèƒ½åˆæˆæ ¸å¿ƒé€»è¾‘"""

        if not self.editor.moviepy_available:
            raise ImportError("moviepyæœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install moviepy")

        from moviepy import (
            ImageClip, VideoFileClip, TextClip, CompositeVideoClip,
            concatenate_videoclips, AudioFileClip
        )
        from moviepy.video.fx import Loop, FadeIn, CrossFadeIn
        from moviepy.audio.fx import AudioLoop

        print(f"\nğŸ¬ æ™ºèƒ½è§†é¢‘åˆæˆ: {script.get('title', 'æœªå‘½å')}")
        print("=" * 70)

        sections = script.get('sections', [])
        if not sections:
            raise ValueError("è„šæœ¬æ²¡æœ‰ç« èŠ‚å†…å®¹")

        # ========== ç¬¬1æ­¥: AIè¯­ä¹‰åˆ†æ ==========
        print(f"\nğŸ§  AIè¯­ä¹‰åˆ†æä¸­...")
        analyses = self.semantic_analyzer.analyze_all_sections(sections)

        # æ‰“å°åˆ†æç»“æœæ‘˜è¦
        print(f"\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
        for i, (section, analysis) in enumerate(zip(sections, analyses), 1):
            print(f"   {i}. {section.get('section_name', 'N/A')}: "
                  f"èƒ½é‡{analysis['energy_level']:.1f}, "
                  f"æƒ…ç»ª{analysis['emotion']}")

        # ========== ç¬¬2æ­¥: ğŸš€ å¹¶è¡Œç”Ÿæˆå¸¦åŠ¨æ•ˆçš„clips ==========
        all_clips = []
        temp_clips = []
        audio_clips = []

        print(f"\nğŸ¨ ğŸš€ å¹¶è¡Œç”Ÿæˆè§†é¢‘ç‰‡æ®µ...")

        # é¢„å…ˆè·å–æ‰€æœ‰ç´ æè·¯å¾„ï¼ˆå¦‚æœéœ€è¦è‡ªåŠ¨é€‰æ‹©ï¼‰
        section_materials = {}
        if auto_select_materials:
            print("ğŸ” æ‰¹é‡æ¨èç´ æ...")
            with ThreadPoolExecutor(max_workers=min(8, len(sections))) as executor:
                future_to_idx = {
                    executor.submit(self._get_material_for_section, section, True): i
                    for i, section in enumerate(sections)
                }
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        section_materials[idx] = future.result()
                    except Exception as e:
                        print(f"   âš ï¸  ç« èŠ‚ {idx + 1} ç´ ææ¨èå¤±è´¥: {str(e)}")
                        section_materials[idx] = None

        # å¹¶è¡Œåˆ›å»ºè§†é¢‘ç‰‡æ®µï¼ˆä¿ç•™é¡ºåºï¼‰
        clips_dict = {}
        lock = threading.Lock()

        def create_smart_clip(i, section, analysis):
            """æ™ºèƒ½åˆ›å»ºå•ä¸ªè§†é¢‘ç‰‡æ®µ"""
            section_name = section.get('section_name', f'ç« èŠ‚{i+1}')

            # 2.1 è·å–ç´ æ
            if auto_select_materials:
                material_path = section_materials.get(i)
            else:
                material_path = None

            duration = self._parse_duration(
                section.get('duration', self.default_image_duration),
                default=self.default_image_duration
            )

            # 2.2 åˆ›å»ºåŸºç¡€clip
            clip = self._create_base_clip(material_path, duration)

            # 2.3 åº”ç”¨Ken Burnsæ•ˆæœï¼ˆå¦‚æœæ˜¯é™æ€å›¾ç‰‡ï¼‰
            kb_info = ""
            if self.ken_burns_enabled and material_path:
                ext = os.path.splitext(material_path)[1].lower()
                if ext in ['.jpg', '.png', '.jpeg', '.gif']:
                    movement_type = self.ken_burns._decide_movement_type(
                        analysis['energy_level'],
                        analysis['emotion']
                    )
                    clip = self.ken_burns.apply_ken_burns(clip, analysis, duration)
                    kb_info = f" | KB: {movement_type}"

            # 2.4 å†³å®šè½¬åœºï¼ˆé™¤ç¬¬ä¸€ä¸ªclipï¼‰
            trans_info = ""
            if i > 0:
                transition = self.transition_engine.decide_transition(
                    analyses[i-1],
                    analysis,
                    analyses[i+1] if i < len(analyses)-1 else None
                )
                trans_info = f" | è½¬åœº: {transition['type']}"
                # åº”ç”¨è½¬åœº
                clip = self._apply_transition(clip, transition)

            # 2.5 æ·»åŠ æ–‡å­—ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
            narration = section.get('narration', '')
            if self.video_config.get('show_narration_text', False) and narration:
                text_clip = self._create_text_clip(
                    narration,
                    duration=duration,
                    position=('center', 'bottom'),
                    fontsize=self.video_config.get('text_size', 40)
                )
                clip = CompositeVideoClip([clip, text_clip])

            with lock:
                clips_dict[i] = clip
                print(f"   âœ… ç« èŠ‚ {i+1}/{len(sections)}: {section_name}{kb_info}{trans_info}")

            return clip

        # å¹¶è¡Œå¤„ç†æ‰€æœ‰ç« èŠ‚ï¼ˆç”±äºè½¬åœºéœ€è¦å‰åä¿¡æ¯ï¼Œé™ä½å¹¶è¡Œåº¦ï¼‰
        with ThreadPoolExecutor(max_workers=min(3, len(sections))) as executor:
            futures = [
                executor.submit(create_smart_clip, i, section, analyses[i])
                for i, section in enumerate(sections)
            ]
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"   âš ï¸  åˆ›å»ºç‰‡æ®µå¤±è´¥: {str(e)}")

        # æŒ‰é¡ºåºç»„è£…clips
        all_clips = [clips_dict[i] for i in sorted(clips_dict.keys())]

        if not all_clips:
            raise ValueError("æ²¡æœ‰ç”Ÿæˆä»»ä½•è§†é¢‘ç‰‡æ®µ")

        # ========== ç¬¬3æ­¥: åˆå¹¶clips ==========
        print(f"\nğŸï¸  åˆå¹¶ {len(all_clips)} ä¸ªç‰‡æ®µ...")
        final_video = concatenate_videoclips(all_clips, method="compose")

        # ========== ç¬¬4æ­¥: æ·»åŠ éŸ³é¢‘ ==========
        final_video = self._add_audio(
            final_video,
            use_tts_audio,
            tts_metadata_path,
            audio_clips
        )

        # ========== ç¬¬5æ­¥: æ·»åŠ å­—å¹• ==========
        if subtitle_file and os.path.exists(subtitle_file):
            final_video = self._add_subtitles(final_video, subtitle_file)

        # ========== ç¬¬6æ­¥: å¯¼å‡ºè§†é¢‘ ==========
        output_path = self._export_video(
            final_video,
            script,
            output_filename,
            len(all_clips)
        )

        # ========== ç¬¬7æ­¥: æ¸…ç†èµ„æº ==========
        self._cleanup_resources(all_clips, audio_clips, final_video)

        return output_path

    def _get_material_for_section(
        self,
        section: Dict[str, Any],
        auto_select: bool
    ) -> Optional[str]:
        """è·å–ç« èŠ‚ç´ æ"""
        if auto_select:
            print("   ğŸ” æ™ºèƒ½æ¨èç´ æ...")
            recommendations = self.recommender.recommend_for_script_section(
                section,
                limit=3
            )

            if recommendations:
                best_material = recommendations[0]
                material_path = best_material['file_path']
                print(f"   âœ… é€‰æ‹©: {best_material['name']} "
                      f"(åŒ¹é…åº¦: {best_material.get('match_score', 0):.0f}%)")
                return material_path
            else:
                print("   âš ï¸  æœªæ‰¾åˆ°åˆé€‚ç´ æï¼Œä½¿ç”¨é»˜è®¤é»‘å±")
                return None
        else:
            return None

    def _create_base_clip(self, material_path: Optional[str], duration: float):
        """åˆ›å»ºåŸºç¡€clip"""
        from moviepy import ImageClip, VideoFileClip
        from moviepy.video.fx import Loop

        if material_path and os.path.exists(material_path):
            ext = os.path.splitext(material_path)[1].lower()
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:
                clip = ImageClip(material_path).with_duration(duration)
            elif ext in ['.mp4', '.avi', '.mov', '.mkv']:
                video_clip = VideoFileClip(material_path)

                # ç§»é™¤éŸ³é¢‘ï¼ˆå› ä¸ºæˆ‘ä»¬ä¼šä½¿ç”¨TTSéŸ³é¢‘ï¼‰
                video_clip = video_clip.without_audio()

                if video_clip.duration < duration:
                    clip = video_clip.with_effects([Loop(duration=duration)])
                else:
                    clip = video_clip.subclipped(0, duration)
            else:
                print(f"   âš ï¸  ä¸æ”¯æŒçš„æ ¼å¼: {ext}")
                clip = self._create_color_clip((0, 0, 0), duration)
        else:
            clip = self._create_color_clip((0, 0, 0), duration)

        return self._ensure_target_resolution(clip)

    def _apply_transition(self, clip, transition: Dict[str, Any]):
        """åº”ç”¨è½¬åœºæ•ˆæœåˆ°clip"""
        trans_type = transition['type']
        duration = transition['duration']
        params = transition.get('params', {})

        if trans_type == 'fade':
            return clip.with_effects([FadeIn(duration)])
        elif trans_type == 'zoom_in':
            zoom_ratio = params.get('zoom_ratio', 1.3)
            return TransitionLibrary.zoom_in(clip, duration, zoom_ratio)
        elif trans_type == 'zoom_out':
            zoom_ratio = params.get('zoom_ratio', 1.3)
            return TransitionLibrary.zoom_out(clip, duration, zoom_ratio)
        elif trans_type == 'slide_left':
            return TransitionLibrary.slide_in(clip, 'left', duration)
        elif trans_type == 'slide_right':
            return TransitionLibrary.slide_in(clip, 'right', duration)
        elif trans_type == 'hard_cut':
            return clip  # æ— è½¬åœº
        elif trans_type == 'crossfade':
            return clip.with_effects([CrossFadeIn(duration)])
        else:
            return clip

    def _add_audio(
        self,
        video_clip,
        use_tts: bool,
        tts_metadata_path: Optional[str],
        audio_clips: list
    ):
        """æ·»åŠ éŸ³é¢‘ï¼ˆTTSæˆ–BGMï¼‰"""
        from moviepy import AudioFileClip, concatenate_audioclips, CompositeAudioClip

        if use_tts and tts_metadata_path and os.path.exists(tts_metadata_path):
            print("\nğŸ™ï¸  æ·»åŠ TTSè¯­éŸ³...")
            try:
                with open(tts_metadata_path, 'r', encoding='utf-8') as f:
                    tts_metadata = json.load(f)

                audio_files = [item['file_path']
                              for item in tts_metadata.get('audio_files', [])]

                if audio_files:
                    tts_audio_clips = [AudioFileClip(f)
                                      for f in audio_files if os.path.exists(f)]
                    audio_clips.extend(tts_audio_clips)

                    if tts_audio_clips:
                        tts_audio = concatenate_audioclips(tts_audio_clips)

                        # æ·»åŠ BGMèƒŒæ™¯
                        bgm_path = self.video_config.get('default_bgm')
                        if bgm_path and os.path.exists(bgm_path):
                            bgm = AudioFileClip(bgm_path)
                            if bgm.duration < tts_audio.duration:
                                bgm = bgm.with_effects([AudioLoop(duration=tts_audio.duration)])
                            else:
                                bgm = bgm.subclipped(0, tts_audio.duration)
                            bgm = bgm.with_volume_scaled(0.2)
                            final_audio = CompositeAudioClip([tts_audio, bgm])
                        else:
                            final_audio = tts_audio

                        video_clip = video_clip.with_audio(final_audio)
                        print(f"   âœ… TTSéŸ³é¢‘å·²æ·»åŠ  (æ—¶é•¿: {tts_audio.duration:.1f}ç§’)")

                        if video_clip.duration != tts_audio.duration:
                            video_clip = video_clip.with_duration(tts_audio.duration)
            except Exception as e:
                print(f"   âš ï¸  æ·»åŠ TTSå¤±è´¥: {str(e)}")
        else:
            # æ·»åŠ BGM
            bgm_path = self.video_config.get('default_bgm')
            if bgm_path and os.path.exists(bgm_path):
                print("\nğŸµ æ·»åŠ èƒŒæ™¯éŸ³ä¹...")
                try:
                    audio = AudioFileClip(bgm_path)
                    if audio.duration < video_clip.duration:
                        audio = audio.with_effects([AudioLoop(duration=video_clip.duration)])
                    else:
                        audio = audio.subclipped(0, video_clip.duration)
                    video_clip = video_clip.with_audio(audio)
                except Exception as e:
                    print(f"   âš ï¸  æ·»åŠ BGMå¤±è´¥: {str(e)}")

        return video_clip

    def _add_subtitles(self, video_clip, subtitle_file: str):
        """æ·»åŠ å­—å¹•"""
        print(f"\nğŸ“ æ·»åŠ å­—å¹•: {subtitle_file}")
        try:
            from moviepy.video.tools.subtitles import SubtitlesClip
            from moviepy import TextClip, CompositeVideoClip

            def generator(txt):
                return TextClip(
                    text=txt,
                    font_size=self.video_config.get('text_size', 48),
                    color='white',
                    bg_color='black',
                    method='caption',
                    size=(video_clip.w - 200, None)
                )

            subtitles = SubtitlesClip(subtitle_file, generator)
            video_clip = CompositeVideoClip([
                video_clip,
                subtitles.with_position(('center', 'bottom'))
            ])

            print("   âœ… å­—å¹•å·²æ·»åŠ ")
        except Exception as e:
            print(f"   âš ï¸  æ·»åŠ å­—å¹•å¤±è´¥: {str(e)}")

        return video_clip

    def _export_video(
        self,
        video_clip,
        script: Dict[str, Any],
        output_filename: Optional[str],
        clip_count: int
    ) -> str:
        """å¯¼å‡ºè§†é¢‘"""
        if output_filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            script_title = script.get('title', 'video')
            safe_title = "".join(c for c in script_title
                                if c.isalnum() or c in (' ', '-', '_')).strip()
            output_filename = f"{safe_title}_{timestamp}.mp4"

        output_path = os.path.join(self.editor.output_dir, output_filename)

        print(f"\nğŸ’¾ å¯¼å‡ºè§†é¢‘...")

        # æ„å»ºFFmpegå‚æ•°ï¼ˆæ”¯æŒGPUåŠ é€Ÿï¼‰
        codec = self.video_config.get('codec', 'libx264')
        ffmpeg_params = []

        if self.video_config.get('gpu_acceleration', False) and 'nvenc' in codec:
            # NVIDIA GPUåŠ é€Ÿå‚æ•°ï¼ˆNVENCç¼–ç å™¨é€‰é¡¹ï¼‰
            preset = self.video_config.get('gpu_preset', 'p4')
            ffmpeg_params = ['-preset', str(preset)]

            custom_nvenc = self.video_config.get('nvenc_params')
            if isinstance(custom_nvenc, list):
                ffmpeg_params.extend(str(p) for p in custom_nvenc)

            print(f"   ğŸš€ å¯ç”¨GPUåŠ é€Ÿ: {codec} (preset: {preset})")

        # æ„å»ºwrite_videofileå‚æ•°
        write_params = {
            'fps': self.video_config.get('fps', 24),
            'codec': codec,
            'audio_codec': self.video_config.get('audio_codec', 'aac'),
            'threads': self.video_config.get('threads', 4)
        }

        # æ·»åŠ FFmpegå‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        if ffmpeg_params:
            write_params['ffmpeg_params'] = ffmpeg_params

        # å¦‚æœä¸æ˜¯GPUç¼–ç ï¼Œæ·»åŠ preset
        if 'nvenc' not in codec:
            write_params['preset'] = self.video_config.get('preset', 'medium')

        # æ‰§è¡Œå¯¼å‡ºï¼Œå¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°CPUç¼–ç 
        try:
            try:
                from moviepy.video.io import ffmpeg_writer
                print(f"   ğŸ§ª FFmpegäºŒè¿›åˆ¶: {ffmpeg_writer.FFMPEG_BINARY}")
                print(f"   ğŸ§µ FFmpegçº¿ç¨‹æ•°: {write_params.get('threads')}")
            except Exception:
                pass

            video_clip.write_videofile(output_path, **write_params)
        except Exception as e:
            if 'nvenc' in codec.lower():
                print("\nâš ï¸  NVENCå¯¼å‡ºå¤±è´¥ï¼Œå°è¯•å›é€€åˆ°CPUç¼–ç (libx264)...")
                print(f"   âŒ NVENCå¤±è´¥è¯¦æƒ…: {e}")
                try:
                    if os.path.exists(output_path):
                        os.remove(output_path)
                except Exception:
                    pass

                try:
                    write_params.pop('ffmpeg_params', None)
                    write_params['codec'] = 'libx264'
                    write_params['preset'] = self.video_config.get('preset', 'medium')
                    video_clip.write_videofile(output_path, **write_params)
                except Exception:
                    raise e
            else:
                raise

        print(f"\nâœ… æ™ºèƒ½è§†é¢‘åˆæˆå®Œæˆ!")
        print(f"   è¾“å‡º: {output_path}")
        print(f"   æ—¶é•¿: {video_clip.duration:.1f}ç§’")
        print(f"   ç‰‡æ®µæ•°: {clip_count}")

        return output_path

    def _cleanup_resources(self, all_clips: list, audio_clips: list, final_video):
        """æ¸…ç†èµ„æºé˜²æ­¢å†…å­˜æ³„æ¼"""
        print("\nğŸ§¹ æ¸…ç†ä¸´æ—¶èµ„æº...")
        try:
            for clip in all_clips:
                if clip and hasattr(clip, 'close'):
                    try:
                        clip.close()
                    except:
                        pass

            for clip in audio_clips:
                if clip and hasattr(clip, 'close'):
                    try:
                        clip.close()
                    except:
                        pass

            if hasattr(final_video, 'close'):
                try:
                    final_video.close()
                except:
                    pass
        except Exception as e:
            print(f"   âš ï¸  æ¸…ç†æ—¶å‡ºé”™: {str(e)}")

    def _ensure_target_resolution(self, clip):
        """ç¡®ä¿å‰ªè¾‘ç¬¦åˆç›®æ ‡åˆ†è¾¨ç‡ï¼ˆLetterbox é€‚é…ï¼‰ã€‚"""
        try:
            resolution = self.video_config.get('resolution', {'width': 1920, 'height': 1080})
            if isinstance(resolution, dict):
                target_width = int(resolution.get('width', 1920))
                target_height = int(resolution.get('height', 1080))
            else:
                target_width, target_height = resolution

            if not target_width or not target_height:
                return clip

            clip_width, clip_height = clip.size
            if clip_width is None or clip_height is None:
                return clip

            target_width = max(2, (target_width // 2) * 2)
            target_height = max(2, (target_height // 2) * 2)

            if clip_width == target_width and clip_height == target_height:
                return clip

            clip_ratio = clip_width / clip_height
            target_ratio = target_width / target_height

            if clip_ratio >= target_ratio:
                new_width = target_width
                new_height = int(round(target_width / clip_ratio))
            else:
                new_height = target_height
                new_width = int(round(target_height * clip_ratio))

            new_width = max(2, (new_width // 2) * 2)
            new_height = max(2, (new_height // 2) * 2)

            resized_clip = clip.resized(new_size=(new_width, new_height))

            return resized_clip.with_background_color(
                size=(target_width, target_height),
                color=(0, 0, 0),
                pos='center'
            )
        except Exception as e:
            print(f"   âš ï¸  è°ƒæ•´åˆ†è¾¨ç‡å¤±è´¥: {e}")
            return clip
